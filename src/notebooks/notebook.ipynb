{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal Predictor  \n",
    "  \n",
    "---------\n",
    "\n",
    "**Author:** Jose Manuel Gonzalez Fornell\n",
    "\n",
    "**Email:** josemanuelgonzalezfornell@gmail.com\n",
    "\n",
    "---------\n",
    "\n",
    "## Index\n",
    "1. [Objetive](#1)\n",
    "2. [Import libraries](#2)\n",
    "3. [Data Extraction](#3)\n",
    "4. [Transform data](#4)\n",
    "5. [Load data](#5)\n",
    "\n",
    "## Objetive<a id='1'></a>  \n",
    "It is propose to design an application to predict the journal in which a paper is most probable to be published, using the abstract of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries<a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import re\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime, text\n",
    "from sqlalchemy.orm import Session, declarative_base\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils.Extraction_functions as efn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction<a id='3'></a>\n",
    "  \n",
    "The attack on the Pubmed API is carried out, and the data that provides useful information is obtained in a dataframe. The selected data includes: PMID (index), Article Title, Publication Date, Journal, Journal Abbreviation, Authors, Abstract, Keywords, Locator Format, and Locator Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start paper 1/100\n",
      "finish paper 1/100\n",
      "start paper 2/100\n",
      "finish paper 2/100\n",
      "start paper 3/100\n",
      "finish paper 3/100\n",
      "start paper 4/100\n",
      "finish paper 4/100\n",
      "start paper 5/100\n",
      "finish paper 5/100\n",
      "start paper 6/100\n",
      "finish paper 6/100\n",
      "start paper 7/100\n",
      "finish paper 7/100\n",
      "start paper 8/100\n",
      "finish paper 8/100\n",
      "start paper 9/100\n",
      "finish paper 9/100\n",
      "start paper 10/100\n",
      "finish paper 10/100\n",
      "start paper 11/100\n",
      "finish paper 11/100\n",
      "start paper 12/100\n",
      "paper 12/100 ignored\n",
      "start paper 13/100\n",
      "paper 13/100 ignored\n",
      "start paper 14/100\n",
      "finish paper 14/100\n",
      "start paper 15/100\n",
      "finish paper 15/100\n",
      "start paper 16/100\n",
      "finish paper 16/100\n",
      "start paper 17/100\n",
      "finish paper 17/100\n",
      "start paper 18/100\n",
      "finish paper 18/100\n",
      "start paper 19/100\n",
      "finish paper 19/100\n",
      "start paper 20/100\n",
      "finish paper 20/100\n",
      "start paper 21/100\n",
      "finish paper 21/100\n",
      "start paper 22/100\n",
      "finish paper 22/100\n",
      "start paper 23/100\n",
      "finish paper 23/100\n",
      "start paper 24/100\n",
      "finish paper 24/100\n",
      "start paper 25/100\n",
      "paper 25/100 ignored\n",
      "start paper 26/100\n",
      "paper 26/100 ignored\n",
      "start paper 27/100\n",
      "finish paper 27/100\n",
      "start paper 28/100\n",
      "finish paper 28/100\n",
      "start paper 29/100\n",
      "finish paper 29/100\n",
      "start paper 30/100\n",
      "finish paper 30/100\n",
      "start paper 31/100\n",
      "paper 31/100 ignored\n",
      "start paper 32/100\n",
      "finish paper 32/100\n",
      "start paper 33/100\n",
      "finish paper 33/100\n",
      "start paper 34/100\n",
      "paper 34/100 ignored\n",
      "start paper 35/100\n",
      "paper 35/100 ignored\n",
      "start paper 36/100\n",
      "finish paper 36/100\n",
      "start paper 37/100\n",
      "finish paper 37/100\n",
      "start paper 38/100\n",
      "finish paper 38/100\n",
      "start paper 39/100\n",
      "finish paper 39/100\n",
      "start paper 40/100\n",
      "paper 40/100 ignored\n",
      "start paper 41/100\n",
      "finish paper 41/100\n",
      "start paper 42/100\n",
      "paper 42/100 ignored\n",
      "start paper 43/100\n",
      "paper 43/100 ignored\n",
      "start paper 44/100\n",
      "finish paper 44/100\n",
      "start paper 45/100\n",
      "paper 45/100 ignored\n",
      "start paper 46/100\n",
      "paper 46/100 ignored\n",
      "start paper 47/100\n",
      "finish paper 47/100\n",
      "start paper 48/100\n",
      "finish paper 48/100\n",
      "start paper 49/100\n",
      "finish paper 49/100\n",
      "start paper 50/100\n",
      "paper 50/100 ignored\n",
      "start paper 51/100\n",
      "paper 51/100 ignored\n",
      "start paper 52/100\n",
      "finish paper 52/100\n",
      "start paper 53/100\n",
      "paper 53/100 ignored\n",
      "start paper 54/100\n",
      "paper 54/100 ignored\n",
      "start paper 55/100\n",
      "paper 55/100 ignored\n",
      "start paper 56/100\n",
      "finish paper 56/100\n",
      "start paper 57/100\n",
      "paper 57/100 ignored\n",
      "start paper 58/100\n",
      "finish paper 58/100\n",
      "start paper 59/100\n",
      "finish paper 59/100\n",
      "start paper 60/100\n",
      "paper 60/100 ignored\n",
      "start paper 61/100\n",
      "finish paper 61/100\n",
      "start paper 62/100\n",
      "paper 62/100 ignored\n",
      "start paper 63/100\n",
      "finish paper 63/100\n",
      "start paper 64/100\n",
      "paper 64/100 ignored\n",
      "start paper 65/100\n",
      "paper 65/100 ignored\n",
      "start paper 66/100\n",
      "paper 66/100 ignored\n",
      "start paper 67/100\n",
      "paper 67/100 ignored\n",
      "start paper 68/100\n",
      "paper 68/100 ignored\n",
      "start paper 69/100\n",
      "paper 69/100 ignored\n",
      "start paper 70/100\n",
      "paper 70/100 ignored\n",
      "start paper 71/100\n",
      "paper 71/100 ignored\n",
      "start paper 72/100\n",
      "finish paper 72/100\n",
      "start paper 73/100\n",
      "paper 73/100 ignored\n",
      "start paper 74/100\n",
      "paper 74/100 ignored\n",
      "start paper 75/100\n",
      "paper 75/100 ignored\n",
      "start paper 76/100\n",
      "finish paper 76/100\n",
      "start paper 77/100\n",
      "finish paper 77/100\n",
      "start paper 78/100\n",
      "finish paper 78/100\n",
      "start paper 79/100\n",
      "finish paper 79/100\n",
      "start paper 80/100\n",
      "paper 80/100 ignored\n",
      "start paper 81/100\n",
      "paper 81/100 ignored\n",
      "start paper 82/100\n",
      "paper 82/100 ignored\n",
      "start paper 83/100\n",
      "paper 83/100 ignored\n",
      "start paper 84/100\n",
      "paper 84/100 ignored\n",
      "start paper 85/100\n",
      "finish paper 85/100\n",
      "start paper 86/100\n",
      "finish paper 86/100\n",
      "start paper 87/100\n",
      "paper 87/100 ignored\n",
      "start paper 88/100\n",
      "paper 88/100 ignored\n",
      "start paper 89/100\n",
      "paper 89/100 ignored\n",
      "start paper 90/100\n",
      "finish paper 90/100\n",
      "start paper 91/100\n",
      "paper 91/100 ignored\n",
      "start paper 92/100\n",
      "paper 92/100 ignored\n",
      "start paper 93/100\n",
      "paper 93/100 ignored\n",
      "start paper 94/100\n",
      "paper 94/100 ignored\n",
      "start paper 95/100\n",
      "paper 95/100 ignored\n",
      "start paper 96/100\n",
      "finish paper 96/100\n",
      "start paper 97/100\n",
      "paper 97/100 ignored\n",
      "start paper 98/100\n",
      "paper 98/100 ignored\n",
      "start paper 99/100\n",
      "paper 99/100 ignored\n",
      "start paper 100/100\n",
      "paper 100/100 ignored\n"
     ]
    }
   ],
   "source": [
    "df_final = efn.attack_api_pubmed(email=\"josemanuelgonzalezfornell@gmail.com\",\n",
    "                                 init_date=\"2020/01/01\", end_date=\"now\", max_results=100, retmax=100, describe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 52 entries, 38171039 to 38170944\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   Title                52 non-null     object        \n",
      " 1   Date                 52 non-null     datetime64[ns]\n",
      " 2   Journal              52 non-null     object        \n",
      " 3   Journal_abreviation  52 non-null     object        \n",
      " 4   All_authors          52 non-null     object        \n",
      " 5   Abstract             52 non-null     object        \n",
      " 6   Keywords             52 non-null     object        \n",
      " 7   Locator_format       52 non-null     object        \n",
      " 8   Locator_number       52 non-null     object        \n",
      "dtypes: datetime64[ns](1), object(8)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data<a id='4'></a>\n",
    "  \n",
    "The data are transformed to have an adequate format. The keywords and authors are transformed into a list of strings separated by commas. The links in the abstract are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Authors column\n",
    "df_final[\"All_authors\"] = df_final[\"All_authors\"].apply(\n",
    "    lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Format Keywords column\n",
    "df_final[\"Keywords\"] = df_final[\"Keywords\"].apply(\n",
    "    lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Format Abstract column\n",
    "df_final[\"Abstract\"] = df_final[\"Abstract\"].apply(lambda x: re.sub(\n",
    "    r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data<a id='5'></a>  \n",
    "  \n",
    "The transformed data are loaded into a database using SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index([38171008, 38171010, 38171011, 38171012, 38171013, 38171016, 38171017,\n       38171018, 38171019, 38171020, 38171021, 38171022, 38171023, 38171024,\n       38171025, 38171026, 38171029, 38171030, 38171031, 38171032, 38171033,\n       38171034, 38171035, 38171036, 38171037, 38171038, 38171039, 38170944,\n       38170950, 38170954, 38170955, 38170961, 38170962, 38170963, 38170964,\n       38170968, 38170977, 38170979, 38170981, 38170982, 38170984, 38170988,\n       38170991, 38170992, 38170993, 38170996, 38170999, 38171001, 38171002,\n       38171003, 38171004, 38171007],\n      dtype='int64')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r1/hd_91nzn1yb8kh1vq1c7gt2w0000gn/T/ipykernel_19758/3427534178.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'sqlite:///../data/processed/Pubmed_DDBB.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM Main\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6563\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6564\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6566\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6568\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6694\u001b[0m         \u001b[0;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6695\u001b[0m         \u001b[0;31m# key that doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6696\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6698\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m             \u001b[0;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Index([38171008, 38171010, 38171011, 38171012, 38171013, 38171016, 38171017,\n       38171018, 38171019, 38171020, 38171021, 38171022, 38171023, 38171024,\n       38171025, 38171026, 38171029, 38171030, 38171031, 38171032, 38171033,\n       38171034, 38171035, 38171036, 38171037, 38171038, 38171039, 38170944,\n       38170950, 38170954, 38170955, 38170961, 38170962, 38170963, 38170964,\n       38170968, 38170977, 38170979, 38170981, 38170982, 38170984, 38170988,\n       38170991, 38170992, 38170993, 38170996, 38170999, 38171001, 38171002,\n       38171003, 38171004, 38171007],\n      dtype='int64')"
     ]
    }
   ],
   "source": [
    "# Create the database\n",
    "engine = create_engine(f'sqlite:///../data/processed/Pubmed_DDBB.db')\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the table Main\n",
    "class Main(Base):\n",
    "    __tablename__ = 'Main'\n",
    "    PMID = Column(Integer, primary_key=True, )\n",
    "    Title = Column(String)\n",
    "    Date = Column(DateTime)\n",
    "    Journal = Column(String)\n",
    "    Journal_abreviation = Column(String)\n",
    "    All_authors = Column(String)\n",
    "    Abstract = Column(String)\n",
    "    Keywords = Column(String)\n",
    "    Locator_format = Column(String)\n",
    "    Locator_number = Column(String)\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "#TODO arreglar para que no se añadan duplicados\n",
    "\n",
    "# Insert the dataframe into the database\n",
    "df_final.to_sql('Main', con=engine, index=True, if_exists='replace')\n",
    "engine = create_engine(f'sqlite:///../data/processed/Pubmed_DDBB.db')\n",
    "session = Session(engine)\n",
    "query = text(\"SELECT * FROM Main\")\n",
    "result = session.execute(query).fetchall()\n",
    "df_final = df_final.concat(results)\n",
    "df_final.drop_duplicates(subset=\"PMID\", keep=\"first\", inplace=True)\n",
    "df_final.to_sql('Main', con=engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data  \n",
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'sqlite:///../data/processed/Pubmed_DDBB.db')\n",
    "session = Session(engine)\n",
    "\n",
    "query = text(\"SELECT PMID, Abstract, Journal FROM Main Limit 100\")\n",
    "\n",
    "result = session.execute(query).fetchall()\n",
    "df_abstract = pd.DataFrame(result).set_index(\"PMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_abstract = df_abstract[df_abstract.groupby('Journal')['Journal'].transform('count') > 1]\n",
    "le = LabelEncoder()\n",
    "df_abstract['Journal'] = le.fit_transform(df_abstract['Journal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_abstract[\"Abstract\"], df_abstract[\"Journal\"], test_size=0.40, random_state=42, stratify=df_abstract[\"Journal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding=\"longest\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_test.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding=\"longest\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_test.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\",\n",
    "                                                      num_labels=df_abstract[\"Journal\"].nunique(),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = le.inverse_transform(df_abstract.loc[:, \"Journal\"])\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bc52b846e04f63a42cbb1ea30f658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8402660bae014df7aee4b8519579a988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([13, 11,  1, 17, 14])\n",
      "tensor([[  101,  1999,  2344,  ...,     0,     0,     0],\n",
      "        [  101, 25416, 26884,  ...,  4022,  2004,   102],\n",
      "        [  101, 22602, 27144,  ...,     0,     0,     0],\n",
      "        [  101, 20310,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2817,  ...,  8534,  2021,   102]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 4,  7, 12, 11, 18])\n",
      "tensor([[  101,  2000,  4503,  ...,     0,     0,     0],\n",
      "        [  101,  5423,  3593,  ...,  1006,  2030,   102],\n",
      "        [  101,  2312,  2653,  ...,     0,     0,     0],\n",
      "        [  101, 26180,  2003,  ..., 26180,  1012,   102],\n",
      "        [  101, 10210, 23924,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 5, 13,  8,  2,  1])\n",
      "tensor([[ 101, 2023, 2817,  ...,    0,    0,    0],\n",
      "        [ 101, 1041, 1011,  ..., 9886, 1997,  102],\n",
      "        [ 101, 6740, 3853,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2817,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 3739,  ...,    0,    0,    0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 8, 15,  1,  5, 15])\n",
      "tensor([[  101,  2287,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,   102,     0,     0],\n",
      "        [  101,  9253,  4215,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 16007,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 0, 20,  9, 20,  6])\n",
      "tensor([[  101,  3384, 11085,  ...,     0,     0,     0],\n",
      "        [  101, 16021,  7934,  ...,     0,     0,     0],\n",
      "        [  101, 20415,  6740,  ...,     0,     0,     0],\n",
      "        [  101,  5372,  4013,  ...,     0,     0,     0],\n",
      "        [  101,  2007,  1996,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 0,  3, 19,  3,  2])\n",
      "tensor([[  101, 22157,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 12490,  ...,     0,     0,     0],\n",
      "        [  101,  5776,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 21766,  7629,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 11649,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([16,  5, 18,  2,  2])\n",
      "tensor([[  101,  2195, 23130,  ...,     0,     0,     0],\n",
      "        [  101,  2144,  2049,  ...,     0,     0,     0],\n",
      "        [  101,  6887,  7911,  ...,     0,     0,     0],\n",
      "        [  101,  2182,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2116,  ...,     0,     0,     0]])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "tensor([10])\n",
      "tensor([[  101, 14163, 27108,  8029, 13186,  2483,  1997,  1996,  2132,  1998,\n",
      "          3300,  2555,  2003,  1037,  4678,  2021,  9376, 28079,  8985,  2008,\n",
      "          2788,  7336, 10047, 23041, 24163,  8737, 21716,  5084,  5022,  1012,\n",
      "          2062,  1999, 19699,  2063, 15417,  2135,  1010,  2023,  8985,  2064,\n",
      "          2036,  5258,  1999,  2111,  2007,  2053,  4728,  2124, 10318, 10047,\n",
      "         23041, 10091, 15074,  1012,  2023, 10958, 15780,  2788,  5320,  1037,\n",
      "          8536,  1999, 11616,  1998,  2089,  8949,  9885,  1996,  3382,  1997,\n",
      "          7691,  1999,  2122,  5022,  1012,  1999,  2023,  2817,  1010,  2057,\n",
      "          2556,  2019,  6034,  2553,  1997, 14163, 27108,  8029, 13186,  2483,\n",
      "          1999,  2019, 10047, 23041, 24163,  8737, 12870,  3372,  5776,  1012,\n",
      "          2011,  9283,  1037, 16030,  3319,  1997,  1996,  3906,  1010,  2057,\n",
      "          6614,  2000,  3623,  2256,  3716,  2006,  2023,  3043,  1012,  2256,\n",
      "          3125,  2003,  2000,  5335, 11616,  1998,  2707,  3949,  2012,  2019,\n",
      "          3041,  2754,  1012,  2256,  5776,  2001,  1037,  2861,  1011,  2095,\n",
      "          1011,  2214,  2158,  2040,  3591,  2007, 17758,  2227, 15903,  2791,\n",
      "          1010,  3300,  3255,  1010, 14978,  1010,  1998,  1037, 26785, 21709,\n",
      "          2594, 14412, 27815,  4649,  3258,  3429,  2420,  2044,  1037, 11394,\n",
      "          7117,  5033,  7709,  1012,  2045,  2001,  4866,  6624,  1997, 13268,\n",
      "          1998,  7412,  2918, 22678,  1998,  3730, 14095,  1012,  2083,  2048,\n",
      "          2139, 23736,  3207,  3672,  6521,  1998, 26721,  8159,  3560,  3424,\n",
      "         11263, 13807,  2140,  3949,  1010,  1996,  5776,  2001, 14374,  2007,\n",
      "          2379,  1011,  3143,  4295,  5813,  1012,  2057,  4453,  4466,  3572,\n",
      "          1999,  1996,  3906,  2008, 10349,  2256,  2817,  9181,  1012,  2057,\n",
      "          9022,  1996,  2783,  3906,  2005, 10003,  3572,  1997, 14163, 27108,\n",
      "          8029, 13186,  2483,  1999,  1996,  2132,  1998,  3300,  2555,  2040,\n",
      "          2134,  1005,  1056,  2031,  2151, 10318,  4295,  1012,  2057, 15901,\n",
      "          2037,  2951,  1998,  2794,  1996,  2951,  1997,  2256,  5776,  1012,\n",
      "          2059,  1010,  2057,  2128,  1011, 16578,  2068,  2478, 22726,  4106,\n",
      "          1010,  9610,  1011,  2675,  1010,  1998, 12441,  8833,  6553, 26237,\n",
      "          2000,  2488,  3305,  1996,  2367,  5876,  2005,  7691,  1998,  4295,\n",
      "         10859,  1999,  2122,  5022,  1012,  4749,  5022,  2020, 16578,  1999,\n",
      "          2023,  2817,  1012,  1996,  2812,  2287,  2001,  4805,  1012,  6109,\n",
      "          1081,  2321,  1012,  4293,  1006,  8117,  2385,  1998,  4098,  2581,\n",
      "          2620,  2086,  2214,  1007,  1012,  1996,  2087, 15157,  4942, 28032,\n",
      "          2063,  2000,  2022,  2920,  2001,  1996, 19432,  1011, 19077, 14163,\n",
      "         13186,  2050,  1010,  2628,  2011,  1996,  4193,  3730, 14095,  1998,\n",
      "          1996,  8753,  1012,  2096,  2119,  8753,  1998, 26721, 26775, 27532,\n",
      "          8153,  6624, 19541,  6022,  2090,  6405,  1998, 10181,  5022,  1010,\n",
      "          2069, 26721, 26775, 27532,  8153,  6624,  2071,  2022,  2109,  2000,\n",
      "         16014,  7691,  1012,  1996,  3452,  7691,  3446,  2001,  6205,  1012,\n",
      "          1022,  1003,  1012,  2348,  2200,  4678,  1010, 14163, 27108,  8029,\n",
      "         13186,  2483,  2064,  5258,  1999, 10047, 23041, 24163,  8737, 12870,\n",
      "          3372,  5022,  1012, 11572,  2323,  5136, 14163, 27108,  8029, 13186,\n",
      "          2483,  2043,  4320,  2007, 25416, 22648,  7062,  3785,  1998,  5866,\n",
      "          8030,  2107,  2004,  6086,  5944,  1010, 13268, 15903,  2791,  1010,\n",
      "         14978,  2015,  1010,  1998, 26721, 23576,  3255,  1012, 21053, 12126,\n",
      "          1006, 14931, 13594,  2007,  2030,  2302, 27011,  1007,  1998,  2010,\n",
      "         14399,  8988, 10091,  7749,  2024,  4187,  2005, 23259, 11616,  2030,\n",
      "         15945,  1997,  2023,  9280, 10611,  2664,  7438,  3085,  4295,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Epoch 1\n",
      "Training loss: 3.2746269404888153\n",
      "Validation loss: 3.06105055809021\n",
      "F1 Score (Weighted): 0.012820512820512822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3eb0cc7b61472b81ff95d7d5fb5a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([11,  5, 12,  0, 15])\n",
      "tensor([[  101, 26180,  2003,  ..., 26180,  1012,   102],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101,  2312,  2653,  ...,     0,     0,     0],\n",
      "        [  101, 22157,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,   102,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 7,  3,  4,  2, 20])\n",
      "tensor([[  101,  5423,  3593,  ...,  1006,  2030,   102],\n",
      "        [  101, 21766,  7629,  ...,     0,     0,     0],\n",
      "        [  101,  2000,  4503,  ...,     0,     0,     0],\n",
      "        [  101,  2182,  1010,  ...,     0,     0,     0],\n",
      "        [  101, 16021,  7934,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 9, 18, 15, 13, 20])\n",
      "tensor([[  101, 20415,  6740,  ...,     0,     0,     0],\n",
      "        [  101, 10210, 23924,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 16007,  ...,     0,     0,     0],\n",
      "        [  101,  1041,  1011,  ...,  9886,  1997,   102],\n",
      "        [  101,  5372,  4013,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([17,  1,  8,  2,  0])\n",
      "tensor([[  101, 20310,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  3739,  ...,     0,     0,     0],\n",
      "        [  101,  2287,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 11649,  ...,     0,     0,     0],\n",
      "        [  101,  3384, 11085,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 2,  5, 19, 16, 11])\n",
      "tensor([[  101,  1999,  2116,  ...,     0,     0,     0],\n",
      "        [  101,  2144,  2049,  ...,     0,     0,     0],\n",
      "        [  101,  5776,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2195, 23130,  ...,     0,     0,     0],\n",
      "        [  101, 25416, 26884,  ...,  4022,  2004,   102]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 3, 10,  2, 13, 14])\n",
      "tensor([[  101,  1996, 12490,  ...,     0,     0,     0],\n",
      "        [  101, 14163, 27108,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2344,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2817,  ...,  8534,  2021,   102]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([18,  1,  8,  1,  6])\n",
      "tensor([[  101,  6887,  7911,  ...,     0,     0,     0],\n",
      "        [  101, 22602, 27144,  ...,     0,     0,     0],\n",
      "        [  101,  6740,  3853,  ...,     0,     0,     0],\n",
      "        [  101,  9253,  4215,  ...,     0,     0,     0],\n",
      "        [  101,  2007,  1996,  ...,     0,     0,     0]])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "tensor([5])\n",
      "tensor([[  101,  2023,  2817,  6461,  2000,  8556,  1996,  4022, 10595,  2011,\n",
      "          2029,  1048,  7274,  8516, 23060,  8524,  3366,  2066,  1017,  1006,\n",
      "          8840,  2595,  2140,  2509,  1007, 13531,  1996,  8285, 21890,  6292,\n",
      "          1999, 16480,  4859,  3217, 27321,  1999,  9808,  2618, 10441, 15265,\n",
      "         14778,  2483,  1006,  1051,  2050,  1007,  1010,  4919,  2083,  1996,\n",
      "         13791,  1997, 26524,  4539,  1997,  9680, 24079, 15459,  3375,  1015,\n",
      "          1006, 11047,  2953,  2278,  2487,  1007,  1012,  2000,  5323,  2019,\n",
      "          1051,  2050,  2944,  1010, 11432,  9601, 15099, 13675, 14194, 13143,\n",
      "         25641,  9099, 18491,  1006,  9353,  7096,  1007,  1012, 16480,  4859,\n",
      "          3217, 27321,  2020,  7275,  2013, 11122, 11733,  3351, 14095,  1998,\n",
      "          3226,  2094,  1012,  2530,  1038, 10994,  3436,  2001,  2864,  2000,\n",
      "         14358,  1996,  3670,  1997,  8840,  2595,  2140,  2509,  1010,  1054,\n",
      "          5369,  2497,  1010,  6887,  2891,  8458, 10253, 13490,  1997,  1052,\n",
      "         19841,  2015,  2575,  2243,  1006,  1052,  1011,  1052, 19841,  2015,\n",
      "          2575,  2243,  1010,  1037, 13248, 12115,  1997, 11047,  2953,  2278,\n",
      "          2487,  1007,  1010,  1998,  8285, 21890,  6292, 16387,  1012,  1996,\n",
      "          8285, 21890,  6292,  1997, 16480,  4859,  3217, 27321,  2001,  5159,\n",
      "          2478,  2019, 10047, 23041, 11253,  7630, 20030,  4632,  4710,  1012,\n",
      "          1996,  3670,  3798,  1997,  2119,  8840,  2595,  2140,  2509,  1998,\n",
      "          1054,  5369,  2497,  8171,  2020,  2039,  2890, 24848,  4383,  1999,\n",
      "         16480,  4859,  3217, 27321,  7275,  2013,  1996,  1051,  2050,  2944,\n",
      "         11122, 11733,  3351,  1010,  1999,  7831,  2000,  2216,  2013,  1996,\n",
      "          3671, 11122, 11733,  3351,  1012,  1996,  9033,  7770,  6129,  1997,\n",
      "          8840,  2595,  2140,  2509,  4504,  1999,  1037,  9885,  1999,  1996,\n",
      "          5250,  3798,  1997,  1054,  5369,  2497,  1998,  1052,  1011,  1052,\n",
      "         19841,  2015,  2575,  2243,  1010,  2004,  2092,  2004,  2019,  3623,\n",
      "          1999,  1996,  3670,  1997,  8285, 21890,  6292,  1011,  3141,  8171,\n",
      "          1012,  5678,  1010,  1996,  3466,  1997,  8840,  2595,  2140,  2509,\n",
      "          2071,  2022, 11674,  2083,  1996,  9033,  7770,  6129,  1997,  1054,\n",
      "          5369,  2497,  1012,  1996,  3463,  1997,  1996, 10047, 23041, 11253,\n",
      "          7630, 20030,  4632,  4710,  4484,  1996,  4254,  1997,  8840,  2595,\n",
      "          2140,  2509,  1998,  1054,  5369,  2497,  2006, 16480,  4859,  3217,\n",
      "          5666,  2618,  8285, 21890,  6292,  1012,  8840,  2595,  2140,  2509,\n",
      "         26402,  2015, 16480,  4859,  3217,  5666,  2618,  8285, 21890,  6292,\n",
      "          2011,  2552, 17441,  1996,  1054,  5369,  2497,  1998, 11047,  2953,\n",
      "          2278,  2487, 14828, 16910,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Epoch 2\n",
      "Training loss: 3.084526866674423\n",
      "Validation loss: 3.0176766872406007\n",
      "F1 Score (Weighted): 0.014492753623188403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6aefe1ceb2d41608a77d08181dabae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 2,  8, 15, 14,  6])\n",
      "tensor([[ 101, 2023, 2817,  ...,    0,    0,    0],\n",
      "        [ 101, 6740, 3853,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2817,  ...,  102,    0,    0],\n",
      "        [ 101, 1996, 2817,  ..., 8534, 2021,  102],\n",
      "        [ 101, 2007, 1996,  ...,    0,    0,    0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 8, 19, 13, 10,  4])\n",
      "tensor([[  101,  2287,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  5776,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2344,  ...,     0,     0,     0],\n",
      "        [  101, 14163, 27108,  ...,     0,     0,     0],\n",
      "        [  101,  2000,  4503,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([15, 18,  1,  0, 17])\n",
      "tensor([[  101,  1996, 16007,  ...,     0,     0,     0],\n",
      "        [  101,  6887,  7911,  ...,     0,     0,     0],\n",
      "        [  101,  9253,  4215,  ...,     0,     0,     0],\n",
      "        [  101,  3384, 11085,  ...,     0,     0,     0],\n",
      "        [  101, 20310,  1005,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 3,  2,  5, 11, 20])\n",
      "tensor([[  101,  1996, 12490,  ...,     0,     0,     0],\n",
      "        [  101,  2182,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2144,  2049,  ...,     0,     0,     0],\n",
      "        [  101, 26180,  2003,  ..., 26180,  1012,   102],\n",
      "        [  101,  5372,  4013,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([18, 12,  5,  0,  1])\n",
      "tensor([[  101, 10210, 23924,  ...,     0,     0,     0],\n",
      "        [  101,  2312,  2653,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101, 22157,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 22602, 27144,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([20,  5,  2,  3, 13])\n",
      "tensor([[  101, 16021,  7934,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 11649,  ...,     0,     0,     0],\n",
      "        [  101, 21766,  7629,  ...,     0,     0,     0],\n",
      "        [  101,  1041,  1011,  ...,  9886,  1997,   102]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 7,  2, 16, 11,  9])\n",
      "tensor([[  101,  5423,  3593,  ...,  1006,  2030,   102],\n",
      "        [  101,  1999,  2116,  ...,     0,     0,     0],\n",
      "        [  101,  2195, 23130,  ...,     0,     0,     0],\n",
      "        [  101, 25416, 26884,  ...,  4022,  2004,   102],\n",
      "        [  101, 20415,  6740,  ...,     0,     0,     0]])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "tensor([1])\n",
      "tensor([[  101,  1996,  3739,  1997,  1048, 24335,  8458, 13045,  1006,  1048,\n",
      "          2078,  1007, 18804,  9153,  6190,  2003,  1037,  2124,  4997,  4013,\n",
      "         26745, 10074,  5387,  1999, 22524,  4456,  1006,  9353,  1007,  5022,\n",
      "          1012,  2174,  1010,  2747,  1996,  6263,  2193,  1997,  1048,  3619,\n",
      "          3223,  2000, 23613,  5646,  1048,  2078, 11265, 20697,  7730,  2003,\n",
      "          4469, 18155,  4383,  2013,  3609, 22471,  2389,  2913,  1998,  2951,\n",
      "          3563,  2000,  9353,  2003, 11158,  1012,  2057,  6461,  2000,  9375,\n",
      "          1996,  7290,  2193,  1997,  1048,  3619,  3223,  2000, 23613,  2754,\n",
      "          9353,  1998, 14358,  2049,  4254,  2006,  2006, 25778,  8649,  2594,\n",
      "         13105,  1012,  5022,  2007,  2754,  2462,  1011,  3523,  9353,  2013,\n",
      "          1996,  2120,  4456,  7809,  1006, 13316, 18939,  2432,  1011, 10476,\n",
      "          1007, 14996, 11707, 24501, 18491,  2007,  3143,  2592,  2055,  1048,\n",
      "          2078,  7749,  2020,  2443,  1012,  4800, 10755, 19210,  8833,  6553,\n",
      "         26237, 14155,  1996, 10238,  1997,  1048,  2078,  3893,  1006,  1048,\n",
      "         16275,  1007,  4295,  2005,  2367,  3616,  1997,  1048,  3619,  8920,\n",
      "          1012,  4800, 10755, 19210,  9574, 26237,  2015,  2020,  2864,  2011,\n",
      "          1048,  2078,  3570, 20576,  2015,  1010, 10426,  2011,  4013, 26745,\n",
      "         10074,  5876,  1010,  2164,  3694,  1010,  2010,  3406, 27179,  4942,\n",
      "         13874,  1010, 11707,  3921,  1010,  1998,  8832,  4748,  9103, 18941,\n",
      "         22575, 27144,  1012,  3452,  1010,  1017,  1010,  3438,  2475,  5022,\n",
      "          2020,  2443,  1010,  2013,  2029,  1015,  1010,  6185,  2575,  1006,\n",
      "          2654,  1012,  1019,  1003,  1007,  2020,  1048, 16275,  1012, 21534,\n",
      "          2702,  1048,  3619,  2001,  1996,  6263,  2193,  3223,  2302, 10548,\n",
      "         10238,  1997,  1048, 16275,  4102,  2007,  1996,  4431,  4696,  1006,\n",
      "          1609,  2322,  1048,  3619,  1007,  1012,  2561,  1048,  3619,  8920,\n",
      "          2020,  1026,  2184,  1999,  4805,  2575,  1006,  2260,  1012,  1023,\n",
      "          1003,  1007,  5022,  1012,  3991,  3582,  1011,  2039,  2013, 11616,\n",
      "          2001,  4293,  1012,  1018,  2706,  1012,  7989,  2000, 16157,  2012,\n",
      "          2560,  2702,  1048,  3619,  2001,  2019,  2981,  4997,  4013, 26745,\n",
      "         10074,  5387,  2005,  3452,  7691,  1006, 10426, 15559,  6463,  1015,\n",
      "          1012,  4464,  1010,  1052,  1026,  1014,  1012,  5890,  1007,  1012,\n",
      "          1999, 22524, 16298, 24755, 11890,  5740,  2863,  1010, 12843,  1037,\n",
      "          6263,  1997,  2702,  1048,  3619,  2001,  4072,  2000, 18478,  1996,\n",
      "          3891,  1997,  4394,  1048, 16275,  4295,  1998,  2001,  3378,  2007,\n",
      "          5301,  3452,  7691,  6165,  1012,  2000, 10210, 28731,  1996,  3891,\n",
      "          1997, 28616, 26266,  9031,  1010,  2019, 11706,  2193,  1997,  3164,\n",
      "          1048,  3619,  2442,  2022, 14155,  2000,  5646,  1048,  2078,  3570,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Epoch 3\n",
      "Training loss: 2.9538740813732147\n",
      "Validation loss: 2.9962749004364015\n",
      "F1 Score (Weighted): 0.03333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edea09c552b64ef79bd611f235087695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 4, 14,  6, 19, 16])\n",
      "tensor([[  101,  2000,  4503,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2817,  ...,  8534,  2021,   102],\n",
      "        [  101,  2007,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  5776,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2195, 23130,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 2, 10,  7,  0, 20])\n",
      "tensor([[  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101, 14163, 27108,  ...,     0,     0,     0],\n",
      "        [  101,  5423,  3593,  ...,  1006,  2030,   102],\n",
      "        [  101, 22157,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  5372,  4013,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([18, 12,  3,  1,  2])\n",
      "tensor([[  101, 10210, 23924,  ...,     0,     0,     0],\n",
      "        [  101,  2312,  2653,  ...,     0,     0,     0],\n",
      "        [  101, 21766,  7629,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  3739,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2116,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([15, 17,  8,  5, 18])\n",
      "tensor([[  101,  2023,  2817,  ...,   102,     0,     0],\n",
      "        [  101, 20310,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2287,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101,  6887,  7911,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 0,  8, 11,  9,  2])\n",
      "tensor([[  101,  3384, 11085,  ...,     0,     0,     0],\n",
      "        [  101,  6740,  3853,  ...,     0,     0,     0],\n",
      "        [  101, 26180,  2003,  ..., 26180,  1012,   102],\n",
      "        [  101, 20415,  6740,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 11649,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([20, 15, 13,  3,  1])\n",
      "tensor([[  101, 16021,  7934,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 16007,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2344,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 12490,  ...,     0,     0,     0],\n",
      "        [  101, 22602, 27144,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 1,  2, 13,  5,  5])\n",
      "tensor([[ 101, 9253, 4215,  ...,    0,    0,    0],\n",
      "        [ 101, 2182, 1010,  ...,    0,    0,    0],\n",
      "        [ 101, 1041, 1011,  ..., 9886, 1997,  102],\n",
      "        [ 101, 2023, 2817,  ...,    0,    0,    0],\n",
      "        [ 101, 2144, 2049,  ...,    0,    0,    0]])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "tensor([11])\n",
      "tensor([[  101, 25416, 26884,  5970,  2470,  8704,  2000, 15502,  2135,  3653,\n",
      "         16280, 20255,  4697,  5022,  2011,  2037,  4848,  8010,  2005,  2536,\n",
      "          4127,  1997,  5970,  1012,  3522,  9849,  2031,  2419,  2000,  1996,\n",
      "          2458,  1997,  7976,  4454,  1011,  6113, 13792,  1010,  2164,  3698,\n",
      "          4083,  8107,  1010,  2000, 14358, 10831,  1998, 11598,  2147, 12314,\n",
      "          1012,  2312,  2653,  4275,  1006,  2222,  5244,  1007,  2066, 11834,\n",
      "         21600,  2102,  1011,  1018,  1006,  2330,  4886,  6948,  1007,  2031,\n",
      "          6003,  2004,  4022,  2236,  7976,  4454,  5906,  2008,  2064,  6509,\n",
      "          2408,  2536, 12736,  1010,  4298,  2164, 25416, 26884,  5970,  3247,\n",
      "          1011,  2437,  1012,  2174,  1010,  2037,  5025,  9859,  1999,  3653,\n",
      "         16280, 20255,  6026, 25416, 26884,  5970,  5022,  2241,  2006,  2613,\n",
      "          1011,  2088, 11709,  3961, 16655,  2595, 24759, 19574,  1012,  2023,\n",
      "          4654, 24759,  6525,  7062,  2817,  6461,  2000,  9398,  3686, 11834,\n",
      "         21600,  2102,  1011,  1018,  1005,  1055,  9859,  1999,  3653, 16280,\n",
      "         20255,  6026, 25416, 26884,  5970,  5022,  2241,  2006,  4141,  2109,\n",
      "          6612, 11709,  1012,  1996,  3125,  2001,  2000, 14358,  3251, 11834,\n",
      "         21600,  2102,  1011,  1018,  1005,  1055,  2836,  2043,  4937, 20265,\n",
      "         21885,  2075, 14108, 20407,  2003, 12435,  2000,  2216,  2081,  2011,\n",
      "          1037, 25416, 26884,  9431,  1012,  1037,  3722, 12441,  2275,  1997,\n",
      "          7236,  1006,  5776,  7218,  2005,  9138, 25416, 26884,  5970,  2030,\n",
      "          2025,  1007,  2004,  2092,  2004,  1037,  2062,  6851,  2275,  2020,\n",
      "          4102,  1012,  2951,  2013,  2531,  5486,  5022,  2013,  1037, 25416,\n",
      "         26884,  9349,  2020,  2019, 16585,  4328,  5422,  1998, 16578,  1012,\n",
      "         11709,  2443,  2287,  1010,  3348,  1010, 19676, 25416, 25533,  1010,\n",
      "          5107,  9353, 18518,  1010,  1998,  2536,  9781, 15879, 11702,  1998,\n",
      "         29299,  2013,  8040,  8049, 14376,  7630,  2290, 12126,  1012,  2023,\n",
      "          2817,  4102, 11834, 21600,  2102,  1011,  1018,  1005,  1055,  2836,\n",
      "          2007,  1037,  9349,  2937,  1005,  1055,  4937, 20265, 26910,  2015,\n",
      "          2478,  9946,  1164, 19064,  1010,  1037,  9610,  1011,  2675,  3231,\n",
      "          1010,  1037,  6724,  8185,  1010, 10640,  1010, 11718,  1010,  9131,\n",
      "          1010,  1042,  1011,  3556,  1010,  1998,  8393,  4082,  8281,  2181,\n",
      "          2104,  1996,  7774,  1012,  1037,  7778,  2135,  3278,  2512,  3597,\n",
      "          2378, 27082, 15758, 10388,  2001,  2179,  2090, 11834, 21600,  2102,\n",
      "          1011,  1018,  1998,  1996,  9349,  2937,  1005,  1055,  4937, 20265,\n",
      "         26910,  2015,  2007,  1037,  9946,  1164, 19064,  1997,  1014,  1012,\n",
      "          4464,  2683,  2005,  1020,  7236,  1006,  5345,  1003, 25022,  1014,\n",
      "          1012, 17273,  1011,  1014,  1012,  5187,  2581,  1007,  1998,  1014,\n",
      "          1012, 19827,  2005, 12441,  4937, 20265, 26910,  1006,  5345,  1003,\n",
      "         25022,  1014,  1012,  4261,  2475,  1011,  1014,  1012,  6535,  2475,\n",
      "          1007,  1012,  1996,  2944,  3662, 15850, 18549,  1998,  3433, 28436,\n",
      "          1010,  2174,  1012,  1996,  9610,  1011,  2675,  3231,  2006,  1020,\n",
      "          7236,  5393,  2019,  2523,  2090,  1996,  1016,  3446,  2869,  1005,\n",
      "         20611,  1006,  1177, 10701,  1027,  6365,  1012,  1021,  1010,  1052,\n",
      "          1026,  1012, 25604,  1007,  1012,  2182,  1010,  1996, 10640,  2001,\n",
      "          1014,  1012,  6273,  1010, 11718,  1014,  1012,  4293,  1010,  9131,\n",
      "          1014,  1012,  6273,  1010,  1998,  1042,  1011,  3556,  1014,  1012,\n",
      "          3963,  1012,  2005,  1016,  7236,  1010,  1996, 10640,  2001,  1014,\n",
      "          1012,  6070,  1010, 11718,  1014,  1012,  6070,  1010,  9131,  1014,\n",
      "          1012,  6070,  1010,  1042,  1011,  3556,  1014,  1012,  6070,  1010,\n",
      "          1998,  2181,  2104,  1996,  7774,  1014,  1012,  6535,  1012,  2023,\n",
      "          2817,  3936,  2008, 11834, 21600,  2102,  1011,  1018, 10637,  4022,\n",
      "          2004,   102]])\n",
      "\n",
      "Epoch 4\n",
      "Training loss: 2.848820000886917\n",
      "Validation loss: 2.995368003845215\n",
      "F1 Score (Weighted): 0.036111111111111115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf53c0d7e744603a51e39e535e563fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 2,  0,  1, 11,  5])\n",
      "tensor([[  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101, 22157,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 22602, 27144,  ...,     0,     0,     0],\n",
      "        [  101, 26180,  2003,  ..., 26180,  1012,   102],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 3, 10, 14, 13, 15])\n",
      "tensor([[  101, 21766,  7629,  ...,     0,     0,     0],\n",
      "        [  101, 14163, 27108,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2817,  ...,  8534,  2021,   102],\n",
      "        [  101,  1041,  1011,  ...,  9886,  1997,   102],\n",
      "        [  101,  2023,  2817,  ...,   102,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 2,  6, 12,  5, 18])\n",
      "tensor([[  101,  1999,  2116,  ...,     0,     0,     0],\n",
      "        [  101,  2007,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  2312,  2653,  ...,     0,     0,     0],\n",
      "        [  101,  2144,  2049,  ...,     0,     0,     0],\n",
      "        [  101, 10210, 23924,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([19,  9, 20,  2,  8])\n",
      "tensor([[  101,  5776,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 20415,  6740,  ...,     0,     0,     0],\n",
      "        [  101, 16021,  7934,  ...,     0,     0,     0],\n",
      "        [  101,  2182,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2287,  1011,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([8, 2, 1, 5, 0])\n",
      "tensor([[  101,  6740,  3853,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 11649,  ...,     0,     0,     0],\n",
      "        [  101,  9253,  4215,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2817,  ...,     0,     0,     0],\n",
      "        [  101,  3384, 11085,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([15,  3,  7, 20, 16])\n",
      "tensor([[  101,  1996, 16007,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 12490,  ...,     0,     0,     0],\n",
      "        [  101,  5423,  3593,  ...,  1006,  2030,   102],\n",
      "        [  101,  5372,  4013,  ...,     0,     0,     0],\n",
      "        [  101,  2195, 23130,  ...,     0,     0,     0]])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "tensor([ 4,  1, 13, 18, 11])\n",
      "tensor([[  101,  2000,  4503,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  3739,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  2344,  ...,     0,     0,     0],\n",
      "        [  101,  6887,  7911,  ...,     0,     0,     0],\n",
      "        [  101, 25416, 26884,  ...,  4022,  2004,   102]])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "tensor([17])\n",
      "tensor([[  101, 20310,  1005,  1055,  4295,  1006, 22851,  1007,  2038,  1037,\n",
      "          3375,  7403,  4281,  5994,  2119,  4678,  1998,  2691,  7403, 10176,\n",
      "          1012,  2348,  1037,  2235,  7017,  1997,  3572,  2265,  1037,  3154,\n",
      "          2273,  9247,  2937, 12839,  5418,  1010,  2009,  2003,  2172,  2062,\n",
      "          7882,  2000,  6709,  5022,  2040,  2556,  2007,  1037,  3375,  7403,\n",
      "          6337,  1997,  3891, 10176,  2007,  2367, 18976,  1012,  1996,  1096,\n",
      "          1011,  1043,  7630,  3597, 17119, 15878,  7352,  8524,  3366, 16861,\n",
      "          4962,  1006, 16351, 27717,  1007,  2003,  3858,  2004,  1996,  2087,\n",
      "          6976,  7403,  3891,  5387,  2005, 22851,  1998, 24992,  2100,  2303,\n",
      "         28767,  1010, 20868,  6072,  5051, 15277,  1997,  7312,  1997,  1996,\n",
      "          9007,  4023,  2349,  2000,  7403, 10176,  1012,  1999,  1037,  3479,\n",
      "          2522, 27794,  1997, 11827,  5588,  5022,  2007,  6612,  5751,  1997,\n",
      "         22851,  1998,  6878,  7403,  3891,  1010,  2057,  2864,  1996,  7403,\n",
      "          5604,  1997,  1996, 16351, 27717,  4962,  1012,  2004,  2060,  7403,\n",
      "          4978,  2064, 19933,  6612,  2838,  1010,  2057,  2036, 12238,  2005,\n",
      "          3176,  4678, 10176,  1999,  2060, 11265, 10976,  3207,  6914, 25284,\n",
      "          9165,  1998, 14155,  1996,  9706,  8913,  1011,  1159,  8991, 26305,\n",
      "          1997,  1996,  5022,  1012,  1999,  2256,  2522, 27794,  1010,  2057,\n",
      "          4453,  2756, 16351, 27717,  4678,  8349,  1006, 27634,  1007, 11363,\n",
      "          1012,  2041,  1997,  1996,  2416,  2367, 11156, 27634,  2015,  1010,\n",
      "          1996,  3811, 15268,  1041, 21619,  2629,  2243,  1998,  1056, 12740,\n",
      "          2620,  2213, 10176,  2024,  3605,  1997,  1996,  3484,  1997,  2068,\n",
      "          1006,  2570,  2041,  1997,  3590,  1007,  1012,  2093,  5022,  3344,\n",
      "          2048, 16351, 27717, 10176,  1010,  1998,  2019,  3176,  2093,  5022,\n",
      "          3344,  4678, 10176,  1999,  2060, 11265, 10976,  3207,  6914, 25284,\n",
      "          9165,  1006, 15488, 17299,  2487,  1010, 11867,  2290, 14526,  1010,\n",
      "          1998,  1055, 20909,  1007,  1012,  2057,  2106,  2025, 11949,  5966,\n",
      "          1999,  2287,  2012, 14447,  2030,  2060,  6612,  2838,  1997,  1996,\n",
      "          5022,  4755,  2048, 16351, 27717, 10176,  2030,  5022,  4755, 21770,\n",
      "         10624,  9096,  3995,  2271,  9706,  8913,  1011,  1159,  2549,  2035,\n",
      "         12260,  1012,  2057,  2342,  2582,  2913,  2000,  2488,  3305,  1996,\n",
      "          6853,  1997,  6612,  5966,  1999,  2122,  5022,  1010,  2004,  2023,\n",
      "          2071,  2031,  2590, 17261, 13494,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Epoch 5\n",
      "Training loss: 2.821175456047058\n",
      "Validation loss: 2.9963664531707765\n",
      "F1 Score (Weighted): 0.034722222222222224\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "    \n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'../data/processed/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: JMIR medical education\n",
      "Accuracy: 2/2\n",
      "\n",
      "Class: Journal of medical Internet research\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: JMIR formative research\n",
      "Accuracy: 0/2\n",
      "\n",
      "Class: JMIR formative research\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Journal of medical Internet research\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: JMIR medical education\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: JMIR formative research\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Journal of medical Internet research\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Plant & cell physiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Plant & cell physiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Plant & cell physiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Plant & cell physiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Current treatment options in oncology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Current treatment options in oncology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Infection\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Infection\n",
      "Accuracy: 1/1\n",
      "\n",
      "Class: Molecular neurobiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Molecular neurobiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Neurological sciences : official journal of the Italian Neurological Society and of the Italian Society of Clinical Neurophysiology\n",
      "Accuracy: 0/2\n",
      "\n",
      "Class: Neurological sciences : official journal of the Italian Neurological Society and of the Italian Society of Clinical Neurophysiology\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: Angewandte Chemie (International ed. in English)\n",
      "Accuracy: 0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\",\n",
    "                                                      num_labels=df_abstract[\"Journal\"].nunique(),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../data/processed/finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Eliminar links\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
